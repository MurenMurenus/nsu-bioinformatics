1. Ссылку на загруженные прочтения из NCBI SRA:
   [Ссылка](https://www.ncbi.nlm.nih.gov/sra/ERX14300106[accn])
2. Скрипт на bash с реализованным алгоритмом:
```bash
#!/bin/bash

./minimap2 -d hg38_index.mmi hg38.fa.gz

./minimap2 -a hg38_index.mmi reed_genome.fastq.gz > output.sam

res=$(./samtools flagstat output.sam | grep -oE '[0-9]+\.[0-9]+%' -m1 | tr -d '%')
if (( $(echo "$res >= 90" | bc -l) )); then
    echo "$res: OK"
else
    echo "$res: NOT OK"
fi
```
Вывод программы:
```
[M::mm_idx_gen::35.667*1.73] collected minimizers
[M::mm_idx_gen::45.622*1.95] sorted minimizers
[M::main::65.517*1.49] loaded/built the index for 455 target sequence(s)
[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 455
[M::mm_idx_stat::66.121*1.49] distinct minimizers: 100202295 (37.96% are singletons); average occurrences: 5.732; average spacing: 5.587; total length: 3209286105
[M::main] Version: 2.29-r1283
[M::main] CMD: ./minimap2 -d hg38_index.mmi hg38.fa.gz
[M::main] Real time: 66.528 sec; CPU: 98.763 sec; Peak RSS: 10.818 GB
[M::main::8.903*0.91] loaded/built the index for 455 target sequence(s)
[M::mm_mapopt_update::10.044*0.92] mid_occ = 728
[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 455
[M::mm_idx_stat::10.587*0.93] distinct minimizers: 100202295 (37.96% are singletons); average occurrences: 5.732; average spacing: 5.587; total length: 3209286105
[M::worker_pipeline::76.825*2.51] mapped 2017562 sequences
[M::main] Version: 2.29-r1283
[M::main] CMD: ./minimap2 -a hg38_index.mmi reed_genome.fastq.gz
[M::main] Real time: 77.349 sec; CPU: 193.125 sec; Peak RSS: 9.009 GB
94.66: OK
```
3. Результат команды samtools flagstat:
```
3271533 + 0 in total (QC-passed reads + QC-failed reads)
2017562 + 0 primary
1210762 + 0 secondary
43209 + 0 supplementary
0 + 0 duplicates
0 + 0 primary duplicates
3096842 + 0 mapped (94.66% : N/A)
1842871 + 0 primary mapped (91.34% : N/A)
0 + 0 paired in sequencing
0 + 0 read1
0 + 0 read2
0 + 0 properly paired (N/A : N/A)
0 + 0 with itself and mate mapped
0 + 0 singletons (N/A : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ>=5)
```
3. Скрипт разбора файлов с этими результатами:
```
всё в скрипте выше
```
5. *Опционально файлы FASTQ, SAM/BAM, VCF в архивах (большой размер!):
```
несколько гигов прикладывать не буду)
```
6. Инструкцию по развертыванию и установке фреймворка:
```
Просто ссылка (https://docs.prefect.io/v3/get-started/quickstart) на Get started. Быстрые три шага.
```
7. Код любого тестового пайплайна (“Hello world”) на фреймворке:
```python
from prefect import flow, task
import random

@task
def get_customer_ids() -> list[str]:
    # Fetch customer IDs from a database or API
    return [f"customer{n}" for n in random.choices(range(100), k=10)]

@task
def process_customer(customer_id: str) -> str:
    # Process a single customer
    return f"Processed {customer_id}"

@flow
def main() -> list[str]:
    customer_ids = get_customer_ids()
    # Map the process_customer task across all customer IDs
    results = process_customer.map(customer_ids)
    return results


if __name__ == "__main__":
    main()
```
8. Результаты работы пайплайна на фреймворке и лог-файлы:
Тут логи есть в панельке справа в детальной информации о запуске
Hello World:
   ![[first_delpoyment.png]]
   ![[hello_world_pipeline.png]]
   Лог-файл есть для уже полноценного пайплайна
9. Код пайплайна “оценки качества картирования” на фреймворке:
```python
from prefect import flow, task, get_run_logger  
import subprocess  
import re  
import os  
  
  
@task  
def create_minimap2_index(reference_fasta: str, index_output: str) -> str:  
    """  
    Create minimap2 index from reference FASTA file    """    logger = get_run_logger()  
    try:  
        logger.info(f"Starting minimap2 index creation for {reference_fasta}")  
        result = subprocess.run(  
            ["./minimap2", "-d", index_output, reference_fasta],  
            capture_output=True, text=True, check=True  
        )  
        logger.info(f"Minimap2 index created successfully at {index_output}")  
        return index_output  
    except subprocess.CalledProcessError as e:  
        logger.error(f"Failed to create minimap2 index: {e.stderr}")  
        raise  
  
  
@task  
def align_reads(index_file: str, reads_file: str, output_sam: str) -> str:  
    """  
    Align reads to reference using minimap2    """    logger = get_run_logger()  
    try:  
        logger.info(f"Starting alignment with index {index_file} and reads {reads_file}")  
        result = subprocess.run(  
            ["./minimap2", "-a", index_file, reads_file],  
            capture_output=True, text=True, check=True  
        )  
        with open(output_sam, "w") as f:  
            f.write(result.stdout)  
        logger.info(f"Alignment completed, SAM file written to {output_sam}")  
        return output_sam  
    except subprocess.CalledProcessError as e:  
        logger.error(f"Alignment failed: {e.stderr}")  
        raise  
    except IOError as e:  
        logger.error(f"Failed to write SAM file: {e}")  
        raise  
  
  
@task  
def sort_sam_to_bam(sam_file: str, sorted_bam: str) -> str:  
    """  
    Sort SAM file and convert to BAM using samtools sort    """    logger = get_run_logger()  
    try:  
        logger.info(f"Starting SAM to BAM sorting for {sam_file}")  
        result = subprocess.run(  
            ["./samtools", "sort", sam_file, "-o", sorted_bam],  
            capture_output=True, text=True, check=True  
        )  
        logger.info(f"SAM file sorted and converted to BAM at {sorted_bam}")  
        return sorted_bam  
    except subprocess.CalledProcessError as e:  
        logger.error(f"Samtools sort failed: {e.stderr}")  
        raise  
    except IOError as e:  
        logger.error(f"Failed to write sorted BAM file: {e}")  
        raise  
  
  
@task  
def check_alignment_quality(sam_file: str, threshold: float = 90.0) -> str:  
    """  
    Check alignment quality using samtools flagstat    """    logger = get_run_logger()  
    try:  
        logger.info(f"Checking alignment quality for {sam_file}")  
        result = subprocess.run(  
            ["./samtools", "flagstat", sam_file],  
            capture_output=True, text=True, check=True  
        )  
        # Extract percentage of mapped reads  
        match = re.search(r'([0-9]+\.[0-9]+)%', result.stdout)  
        if not match:  
            logger.error("Could not parse alignment percentage from samtools output")  
            raise ValueError("Invalid samtools output format")  
  
        percentage = float(match.group(1))  
        logger.info(f"Alignment percentage: {percentage}%")  
  
        status = "OK" if percentage >= threshold else "NOT OK"  
        logger.info(f"Alignment quality check result: {status} (threshold: {threshold}%)")  
        return f"{percentage}: {status}"  
    except subprocess.CalledProcessError as e:  
        logger.error(f"Samtools flagstat failed: {e.stderr}")  
        raise  
    except ValueError as e:  
        logger.error(f"Error processing alignment percentage: {e}")  
        raise  
  
  
@task  
def extract_reference_fasta(compressed_fasta: str, extracted_fasta: str) -> str:  
    """  
    Extract compressed FASTA file using gunzip    """    logger = get_run_logger()  
    try:  
        logger.info(f"Extracting {compressed_fasta} to {extracted_fasta}")  
        if os.path.exists(extracted_fasta):  
            logger.info(f"{extracted_fasta} already exists, skipping extraction")  
            return extracted_fasta  
        result = subprocess.run(  
            ["gunzip", "-c", "-k",  compressed_fasta],  
            capture_output=True, text=True, check=True  
        )  
        with open(extracted_fasta, "w") as f:  
            f.write(result.stdout)  
        logger.info(f"Successfully extracted {compressed_fasta} to {extracted_fasta}")  
        return extracted_fasta  
    except subprocess.CalledProcessError as e:  
        logger.error(f"Gunzip extraction failed: {e.stderr}")  
        raise  
    except IOError as e:  
        logger.error(f"Failed to write extracted FASTA file: {e}")  
        raise  
  
  
@task  
def run_freebayes(reference_fasta: str, sorted_bam: str, output_vcf: str) -> str:  
    """  
    Run freebayes to generate variant calls    """    logger = get_run_logger()  
    try:  
        logger.info(f"Starting freebayes variant calling with {sorted_bam}")  
        result = subprocess.run(  
            ["./freebayes", "-f", reference_fasta, sorted_bam],  
            capture_output=True, text=True, check=True  
        )  
        with open(output_vcf, "w") as f:  
            f.write(result.stdout)  
        logger.info(f"Freebayes completed, VCF report written to {output_vcf}")  
        return output_vcf  
    except subprocess.CalledProcessError as e:  
        logger.error(f"Freebayes failed: {e.stderr}")  
        raise  
    except IOError as e:  
        logger.error(f"Failed to write VCF file: {e}")  
        raise  
  
  
@flow  
def genome_alignment_pipeline(  
        reference_fasta: str = "hg38.fa.gz",  
        reads_file: str = "reed_genome.fastq.gz",  
        index_output: str = "hg38_index.mmi",  
        output_sam: str = "output.sam",  
        sorted_bam: str = "out.sorted.bam",  
        output_vcf: str = "bayes_report.vcf",  
        extracted_fasta: str = "hg38.fa",  
        threshold: float = 90.0  
) -> str:  
    """DAG init"""  
    logger = get_run_logger()  
    logger.info("Starting genome alignment pipeline")  
  
    index_result = create_minimap2_index(reference_fasta, index_output)  
    sam_result = align_reads(index_result, reads_file, output_sam)  
    quality_result = check_alignment_quality(sam_result, threshold)  
  
    if "OK" in quality_result:  
        bam_result = sort_sam_to_bam(sam_result, sorted_bam)  
        extracted_fasta_result = extract_reference_fasta(reference_fasta, extracted_fasta)  
        vcf_result = run_freebayes(extracted_fasta_result, bam_result, output_vcf)  
        logger.info(f"Final VCF report generated at: {vcf_result}")  
    else:  
        logger.warning(f"Alignment quality is {quality_result}, skipping sorting, extraction, and variant calling")  
  
    logger.info(f"Genome alignment pipeline completed with result: {quality_result}")  
    return quality_result  
  
  
if __name__ == "__main__":  
    result = genome_alignment_pipeline()  
    print(result)
```
8. Выведенные результаты работы пайплайна на загруженных данных в отдельном файле:
```
Извините, но у меня директория со всеми файлами, где исполняется пайплайн, весит 15гб. Даже bayes_report весит больше 25мб. Отчет FastQC приложить могу, FastQC_report.html рядом.
```
8. Лог-файлы работы пайплайна на загруженных данных:
```
pipeline-logs.csv рядом
```
8. Визуализацию пайплайна в виде графического файла:
![[genome_pipeline_overview.png]]
![[run_graph.png]]

9. Описание использованного способа визуализации и отличия полученной визуализации от блок-схемы алгоритма в свободной форме:
Фреймворк позволяет видеть время исполнения каждого этапа, в более удобной форме представлять входы/выходы каждого этапа (для программиста json точно удобнее блок схемы). Отдельные логи по каждому этапу - вообще отдельное преимущество, перекрывающее всё.